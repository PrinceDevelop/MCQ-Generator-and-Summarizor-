{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvovOkJ85ecEbfWxiqN1Bd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPS-7-LejsF4"
      },
      "outputs": [],
      "source": [
        "!pip install gradio transformers sentence-transformers PyPDF2 python-docx torch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx python-pptx PyPDF2"
      ],
      "metadata": {
        "id": "ecGKsG9Ckr-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# üß† AI MCQ + Summarizer + Copilot (CPU Safe Mode) ‚Äî FIXED\n",
        "# ==========================================\n",
        "\n",
        "# Install (only if needed). In notebook remove the ! if already installed.\n",
        "#!pip install -q gradio python-docx PyPDF2 transformers sentence-transformers torch nest_asyncio scikit-learn\n",
        "\n",
        "import gradio as gr\n",
        "import os, re, random, torch, docx, PyPDF2, warnings, asyncio, nest_asyncio\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ----------------------- FIX EVENT LOOP -----------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nest_asyncio.apply()\n",
        "asyncio.set_event_loop(asyncio.new_event_loop())\n",
        "\n",
        "# ----------------------- FORCE CPU -----------------------\n",
        "device = \"cpu\"\n",
        "torch.device(device)\n",
        "print(\"‚úÖ Running in CPU Safe Mode\")\n",
        "\n",
        "# ----------------------- LOAD MODELS ------------------------\n",
        "QG_MODEL = \"iarfmoose/t5-base-question-generator\"\n",
        "qg_tokenizer = AutoTokenizer.from_pretrained(QG_MODEL)\n",
        "qg_model = AutoModelForSeq2SeqLM.from_pretrained(QG_MODEL).to(device)\n",
        "\n",
        "emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
        "\n",
        "# ----------------------- HELPERS ------------------------\n",
        "def clean_text(txt):\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt or \"\")\n",
        "    txt = re.sub(r\"[?]{2,}\", \"?\", txt)\n",
        "    return txt.strip()\n",
        "\n",
        "def extract_sentences(text):\n",
        "    return [s.strip() for s in re.split(r'(?<=[.!?]) +', text) if s.strip()] or [text]\n",
        "\n",
        "def read_file(file):\n",
        "    \"\"\"Read TXT, DOCX, or PDF files safely and return extracted text (string).\"\"\"\n",
        "    if not file:\n",
        "        return \"\"\n",
        "    ext = os.path.splitext(file.name)[1].lower()\n",
        "    text = \"\"\n",
        "    try:\n",
        "        if ext == \".txt\":\n",
        "            # In Gradio, file supports read()\n",
        "            data = file.read()\n",
        "            if isinstance(data, bytes):\n",
        "                text = data.decode(\"utf-8\", errors=\"ignore\")\n",
        "            else:\n",
        "                text = str(data)\n",
        "        elif ext == \".docx\":\n",
        "            # file is a tempfile-like object; pass it directly to docx\n",
        "            doc = docx.Document(file)\n",
        "            text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "        elif ext == \".pdf\":\n",
        "            # PyPDF2 accepts file-like object\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text() or \"\"\n",
        "                text += page_text + \" \"\n",
        "        else:\n",
        "            return \"‚ö†Ô∏è Unsupported file type. Use .txt, .docx, or .pdf\"\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Error reading file: {e}\"\n",
        "    return clean_text(text)[:8000]\n",
        "\n",
        "# ------------------ Extractive fallback (TF-IDF) ------------------\n",
        "def extractive_summary(text, n_sentences=3):\n",
        "    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
        "    if not sents:\n",
        "        return \"\"\n",
        "    n_sentences = min(len(sents), max(1, int(n_sentences)))\n",
        "    if len(\" \".join(sents)) < 200:\n",
        "        return \" \".join(sents[:n_sentences])\n",
        "    try:\n",
        "        vect = TfidfVectorizer(stop_words=\"english\")\n",
        "        X = vect.fit_transform(sents)\n",
        "        scores = X.sum(axis=1).A1\n",
        "        ranked_idx = scores.argsort()[::-1]\n",
        "        top_idx = sorted(ranked_idx[:n_sentences])\n",
        "        top_sents = [sents[i] for i in top_idx]\n",
        "        return \" \".join(top_sents)\n",
        "    except Exception:\n",
        "        return \" \".join(sents[:n_sentences])\n",
        "\n",
        "# ------------------ Safe abstractive summarizer with chunking ------------------\n",
        "def summarize_text_safe(text, mode=\"Brief\"):\n",
        "    \"\"\"\n",
        "    Safe summarizer:\n",
        "      - validates input length\n",
        "      - uses model with truncation when possible\n",
        "      - chunks long text and summarizes each chunk\n",
        "      - falls back to extractive summary if abstractive fails\n",
        "    \"\"\"\n",
        "    text = (text or \"\").strip()\n",
        "    if not text:\n",
        "        return \"‚ö†Ô∏è Please enter or upload text.\"\n",
        "    if len(text.split()) < 20:\n",
        "        return \"‚ö†Ô∏è Please enter longer text (at least 2‚Äì3 sentences).\"\n",
        "\n",
        "    # model tokenizer max heuristic\n",
        "    try:\n",
        "        model_max = getattr(summarizer.tokenizer, \"model_max_length\", 1024)\n",
        "        if model_max is None or model_max <= 0 or model_max > 4096:\n",
        "            model_max = 1024\n",
        "    except Exception:\n",
        "        model_max = 1024\n",
        "\n",
        "    # set lengths based on mode\n",
        "    if mode == \"Detailed\":\n",
        "        max_len, min_len = 300, 80\n",
        "    else:\n",
        "        max_len, min_len = 150, 30\n",
        "\n",
        "    cleaned = clean_text(text)\n",
        "    # If relatively short, try direct summarization with truncation\n",
        "    try:\n",
        "        if len(cleaned) < model_max * 4:\n",
        "            out = summarizer(cleaned, max_length=max_len, min_length=min_len, do_sample=False, truncation=True)\n",
        "            return out[0][\"summary_text\"]\n",
        "    except Exception:\n",
        "        # try truncating and summarizing portion\n",
        "        try:\n",
        "            short = cleaned[: model_max * 3]\n",
        "            out = summarizer(short, max_length=max_len, min_length=min_len, do_sample=False, truncation=True)\n",
        "            return out[0][\"summary_text\"]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Chunk long text by sentences (~safe char-sized chunks)\n",
        "    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', cleaned) if s.strip()]\n",
        "    chunks = []\n",
        "    cur, cur_len = [], 0\n",
        "    chunk_char_limit = model_max * 3  # heuristic\n",
        "    for s in sentences:\n",
        "        cur.append(s)\n",
        "        cur_len += len(s)\n",
        "        if cur_len >= chunk_char_limit:\n",
        "            chunks.append(\" \".join(cur))\n",
        "            cur, cur_len = [], 0\n",
        "    if cur:\n",
        "        chunks.append(\" \".join(cur))\n",
        "\n",
        "    # Summarize each chunk (with fallback to extractive per chunk)\n",
        "    chunk_summaries = []\n",
        "    for ch in chunks:\n",
        "        try:\n",
        "            out = summarizer(ch, max_length=max_len, min_length=min_len, do_sample=False, truncation=True)\n",
        "            chunk_summaries.append(out[0][\"summary_text\"])\n",
        "        except Exception:\n",
        "            chunk_summaries.append(extractive_summary(ch, n_sentences=2))\n",
        "\n",
        "    combined = \" \".join(chunk_summaries)\n",
        "    if len(combined.split()) > 400:\n",
        "        return extractive_summary(combined, n_sentences=5)\n",
        "    if not combined.strip():\n",
        "        return extractive_summary(text, n_sentences=4)\n",
        "    return combined\n",
        "\n",
        "# ----------------------- MCQ GENERATOR ------------------------\n",
        "def gen_mcqs_from_text(text, n_q=5):\n",
        "    text = (text or \"\").strip()\n",
        "    if not text or len(text.split()) < 20:\n",
        "        return \"‚ö†Ô∏è Please provide more text (upload or paste document).\"\n",
        "\n",
        "    try:\n",
        "        sentences = extract_sentences(text)\n",
        "        passage = \" \".join(sentences[:15])\n",
        "        inputs = qg_tokenizer.encode(f\"generate questions from: {passage}\",\n",
        "                                     return_tensors=\"pt\", truncation=True)\n",
        "        outputs = qg_model.generate(inputs, max_length=128, num_return_sequences=int(n_q),\n",
        "                                    do_sample=True, top_p=0.92, temperature=0.8)\n",
        "        questions = [clean_text(qg_tokenizer.decode(o, skip_special_tokens=True)) for o in outputs]\n",
        "\n",
        "        result = \"\"\n",
        "        for q in questions:\n",
        "            hits = util.semantic_search(emb_model.encode(q, convert_to_tensor=True),\n",
        "                                        emb_model.encode(sentences, convert_to_tensor=True), top_k=4)[0]\n",
        "            opts = [sentences[h[\"corpus_id\"]] for h in hits]\n",
        "            if not opts:\n",
        "                continue\n",
        "            correct = opts[0]\n",
        "            distractors = opts[1:4]\n",
        "            while len(distractors) < 3:\n",
        "                distractors.append(random.choice(sentences))\n",
        "            all_opts = distractors[:3] + [correct]\n",
        "            random.shuffle(all_opts)\n",
        "\n",
        "            result += f\"**Q:** {clean_text(q)}\\n\"\n",
        "            for i, opt in enumerate(all_opts, 1):\n",
        "                result += f\"{i}. {clean_text(opt)}\\n\"\n",
        "            result += f\"‚úÖ **Answer:** {clean_text(correct)}\\n\\n\"\n",
        "        return result or \"‚ö†Ô∏è No questions generated from the given text.\"\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Error generating MCQs: {e}\"\n",
        "\n",
        "# ----------------------- COPILOT CHATBOT ------------------------\n",
        "def copilot_reply(message, history):\n",
        "    if not message.strip():\n",
        "        return \"Hi üëã Ask me anything!\"\n",
        "    if \"mcq\" in message.lower():\n",
        "        return \"üß© I can generate MCQs! Go to the MCQ Generator tab and upload your document.\"\n",
        "    if \"summary\" in message.lower():\n",
        "        return \"üìù I can summarize text ‚Äî go to the Summarizer tab or paste text here.\"\n",
        "    return \"ü§ñ Copilot: I can summarize, generate MCQs, or explain your content!\"\n",
        "\n",
        "# ----------------------- UI ------------------------\n",
        "def build_app():\n",
        "    theme = gr.themes.Soft(primary_hue=\"violet\", neutral_hue=\"gray\").set(\n",
        "        body_background_fill=\"#0f172a\",\n",
        "        body_text_color=\"#e2e8f0\",\n",
        "        block_background_fill=\"#1e293b\"\n",
        "    )\n",
        "\n",
        "    with gr.Blocks(theme=theme, title=\"AI MCQ Generator APP\") as demo:\n",
        "        gr.Markdown(\"<h1 style='text-align:center;'>AI MCQ Generator & Summarizer</h1>\")\n",
        "\n",
        "        # ---------- MCQ GENERATOR TAB ----------\n",
        "        with gr.Tab(\"MCQ Generator\"):\n",
        "            with gr.Row():\n",
        "                file_input = gr.File(label=\"Upload File (.txt, .docx, .pdf)\")\n",
        "                n_questions = gr.Slider(1, 100, value=5, step=1, label=\"Number of Questions\")\n",
        "            text_box = gr.Textbox(label=\"Extracted Text\", lines=10, placeholder=\"File text will appear here...\")\n",
        "            gen_btn = gr.Button(\"üöÄ Generate MCQs\", variant=\"primary\")\n",
        "            mcq_output = gr.Markdown(label=\"Generated Questions\")\n",
        "\n",
        "            # wire up events\n",
        "            file_input.change(fn=read_file, inputs=file_input, outputs=text_box)\n",
        "            gen_btn.click(fn=gen_mcqs_from_text, inputs=[text_box, n_questions], outputs=mcq_output, api_name=\"generate_mcqs\")\n",
        "\n",
        "        # ---------- SUMMARIZER TAB ----------\n",
        "        with gr.Tab(\"üìù Summarizer\"):\n",
        "            input_text = gr.Textbox(lines=8, label=\"Enter Text\")\n",
        "            mode_radio = gr.Radio([\"Brief\", \"Detailed\"], value=\"Brief\", label=\"Summary Type\")\n",
        "            summarize_btn = gr.Button(\"‚ú® Summarize\", variant=\"primary\")\n",
        "            summary_output = gr.Textbox(lines=8, label=\"Summary\")\n",
        "            summarize_btn.click(fn=lambda t, m: summarize_text_safe(t, mode=m), inputs=[input_text, mode_radio], outputs=summary_output, api_name=\"summarize\")\n",
        "\n",
        "        # ---------- COPILOT TAB ----------\n",
        "        with gr.Tab(\"ü§ñ Copilot\"):\n",
        "            gr.ChatInterface(\n",
        "                fn=copilot_reply,\n",
        "                type=\"messages\",\n",
        "                title=\"ü§ñ Copilot Chat\",\n",
        "                description=\"Ask me anything or get help summarizing or generating MCQs!\"\n",
        "            )\n",
        "\n",
        "        # ---------- FLOATING CHAT BUTTON ----------\n",
        "        gr.HTML(\"\"\"\n",
        "        <style>\n",
        "        .copilot-btn {\n",
        "            position: fixed; bottom: 25px; right: 25px;\n",
        "            background: linear-gradient(135deg,#4f46e5,#7c3aed);\n",
        "            color: white; border-radius: 50%; width: 60px; height: 60px;\n",
        "            display:flex; align-items:center; justify-content:center;\n",
        "            font-size:28px; cursor:pointer; box-shadow:0 4px 8px rgba(0,0,0,0.3);\n",
        "            transition:all 0.3s;\n",
        "        }\n",
        "        .copilot-btn:hover {transform:scale(1.1);}\n",
        "        </style>\n",
        "        <div class='copilot-btn' onclick=\"\n",
        "        const tabs=document.querySelectorAll('button');\n",
        "        tabs.forEach(btn=>{\n",
        "            if(btn.innerText.includes('Copilot')) btn.click();\n",
        "        });\n",
        "        \">üí¨</div>\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# ----------------------- LAUNCH ------------------------\n",
        "app = build_app()\n",
        "app.launch(share=True, debug=False)\n"
      ],
      "metadata": {
        "id": "0CyiNRkX3ATk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}